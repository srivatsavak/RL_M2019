{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Testbed(object):\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.action_count = np.zeros(k)\n",
    "        self.best_action = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.action_count = np.random.randn(self.k)\n",
    "        self.best_action = np.argmax(self.action_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self,k,epsilon=0):\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def action(self): \n",
    "        if np.random.random() < self.epsilon:\n",
    "            self.A = np.random.choice(len(self.qEstimate))\n",
    "            return self.A\n",
    "        else:  \n",
    "            action = np.where(self.qEstimate == np.argmax(self.qEstimate))[0]\n",
    "            if len(action) == 0:\n",
    "                self.A = np.argmax(self.qEstimate) #probability 1-𝞮\n",
    "                return self.A \n",
    "            else:\n",
    "                self.A = np.random.choice(action) #probability 𝞮\n",
    "                return self.A\n",
    "\n",
    "    def step(self, reward):\n",
    "        a = self.A\n",
    "        self.𝜶_k[a] += 1  \n",
    "        self.qEstimate[a] += (reward - self.qEstimate[a])/self.𝜶_k[a] #Sample averages - calculating new estimate with step-size parameter 1/n\n",
    "\n",
    "    def reset(self):\n",
    "        self.A = None\n",
    "        self.𝜶_k = np.zeros(self.k)\n",
    "        self.qEstimate = np.zeros(self.k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(testbed, agent, timeSteps, runs):\n",
    "    rewards = np.array([0.]*timeSteps)\n",
    "    best_action = np.array([0.]*timeSteps)\n",
    "    for i in range(runs):\n",
    "        if (i%500) == 0:\n",
    "            print(str(runs-i)+\" steps remaining\")\n",
    "        testbed.reset() #reset the test bed for each iteration\n",
    "        agent.reset() #reset the agent for each iteration\n",
    "        for j in range(timeSteps):\n",
    "            A_t = agent.action() #action at time step t \n",
    "            R_t = np.random.normal(testbed.action_count[A_t], 1) #actual reward selected from normal distribution with mean q*(A_t) and variance 1\n",
    "            agent.step(R_t) #incremental steps for updating averages\n",
    "            rewards[j] += R_t \n",
    "            if A_t == testbed.best_action:\n",
    "                best_action[j] += 1\n",
    "    return rewards/runs, best_action/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 steps remaining\n",
      "1500 steps remaining\n",
      "1000 steps remaining\n",
      "500 steps remaining\n",
      "2000 steps remaining\n",
      "1500 steps remaining\n",
      "1000 steps remaining\n",
      "500 steps remaining\n",
      "2000 steps remaining\n",
      "1500 steps remaining\n",
      "1000 steps remaining\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "runs = 2000\n",
    "timeSteps = 1000\n",
    "epsilons = [0,0.01,0.1]\n",
    "avg_reward = []\n",
    "optimal_action = []\n",
    "\n",
    "for eps in epsilons:\n",
    "    a, b = simulate(Testbed(k), Agent(k,eps), timeSteps, runs)\n",
    "    avg_reward.append(a.tolist())\n",
    "    optimal_action.append(b.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_reward[0], label=r'$\\varepsilon$ = 0')\n",
    "plt.plot(avg_reward[1], label=r'$\\varepsilon$ = 0.01')\n",
    "plt.plot(avg_reward[2], label=r'$\\varepsilon$ = 0.1')\n",
    "plt.legend(loc=4)\n",
    "plt.ylabel('Average reward')\n",
    "plt.xlabel('Steps')\n",
    "plt.savefig('avg_2_5.jpg')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(optimal_action[0], label=r'$\\varepsilon$ = 0')\n",
    "plt.plot(optimal_action[1], label=r'$\\varepsilon$ = 0.01')\n",
    "plt.plot(optimal_action[2], label=r'$\\varepsilon$ = 0.1')\n",
    "plt.ylim(0,1)\n",
    "plt.legend(loc=4)\n",
    "plt.ylabel('% Optimal action')\n",
    "plt.savefig('opt_2_5.jpg')\n",
    "plt.xlabel('Steps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
